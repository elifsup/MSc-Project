{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6073420-f57d-4b42-80e6-f8f162b54440",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c101fc68-5622-44df-804b-362fcb12a178",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import numpy as np\n",
    "import hashlib\n",
    "from metrics import cleanup_result_first_new, cleanup_result_last_new, cleanup_result_after_result, metrics_mine_dict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\n",
    "import random\n",
    "import seaborn as sns\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80907552-b6ed-4f1a-9ce0-7916973523d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('test_dataset.csv')\n",
    "\n",
    "test_clauses = df_test['clause'].tolist()\n",
    "test_risks = df_test['ground_truth_label'].tolist()\n",
    "test_contract_types = df_test['contract_type'].tolist()\n",
    "test_representing = df_test['representing'].tolist()\n",
    "test_sources = df_test['source'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f64c004-db6b-4956-b140-57fff305744f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "folder_path = 'results'\n",
    "\n",
    "results = []\n",
    "\n",
    "pickle_files = sorted([f for f in os.listdir(folder_path) if f.endswith('.pkl')])\n",
    "\n",
    "for filename in pickle_files:\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    with open(file_path, 'rb') as file:\n",
    "        result = pickle.load(file)\n",
    "        results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7d198e-8d64-44b3-b8b8-6e6615b08d16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_text_files(directory):\n",
    "    \"\"\"\n",
    "    function to create a dictionary whose keys are the source document\n",
    "    identifier and the values are the texts of the source documents\n",
    "    \"\"\"\n",
    "    text_files = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "                # Remove the '.txt' extension from the filename\n",
    "                key_name = filename.rstrip('.txt')\n",
    "                text_files[key_name] = file.read()\n",
    "    return text_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6d20f3-bcb9-472a-b976-1abd964851c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## updated results and summary dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9347ce2f-3bf2-49c4-8bf1-1bc631a9c615",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N = 200\n",
    "\n",
    "updated_results = []\n",
    "for i in range(len(results)):\n",
    "    dataframe = pd.DataFrame(results[i])\n",
    "    experiment_name = dataframe['experiment_name'][0]\n",
    "    if 'step by step' in experiment_name or 'appending' in experiment_name:\n",
    "        dataframe['better_cleaned'] = dataframe['model_output'].apply(cleanup_result_first_new)\n",
    "    elif 'alex' in experiment_name:\n",
    "        dataframe['better_cleaned'] = dataframe['model_output'].apply(cleanup_result_after_result)\n",
    "        \n",
    "    else:\n",
    "        dataframe['better_cleaned'] = dataframe['model_output'].apply(cleanup_result_last_new)\n",
    "    \n",
    "    new_cleaned_results = dataframe['better_cleaned'].tolist()\n",
    "    \n",
    "    new_metrics = metrics_mine_dict(test_risks[:N], new_cleaned_results)\n",
    "    \n",
    "    dataframe['accuracy_new'] = new_metrics['Accuracy']\n",
    "    dataframe['precision_new'] = new_metrics['Precision']\n",
    "    dataframe['recall_new'] = new_metrics['Recall']\n",
    "    dataframe['F1_new'] = new_metrics['F1']\n",
    "       \n",
    "    updated_results.append(dataframe)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ed27cb-5a0a-4bed-856c-b18544a14774",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#columns_to_consider = ['experiment_name', 'model', 'F1_new','total cost of this experiment','experiment ran for (seconds)']\n",
    "columns_to_consider = ['experiment_name', 'model', 'F1_new','experiment ran for (seconds)']\n",
    "unique_values = {col: [] for col in columns_to_consider}\n",
    "\n",
    "# Iterate through each DataFrame and collect unique values\n",
    "for df in updated_results:\n",
    "    for col in columns_to_consider:\n",
    "        unique_val = df[col].unique()\n",
    "        if len(unique_val) == 1:  # Ensure it's a unique value column\n",
    "            unique_values[col].append(unique_val[0])\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "summary = pd.DataFrame(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec39e49-6420-4c1e-abc5-7af0468097c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb14f6d-4d4a-449f-a4dd-ebb8c42d2769",
   "metadata": {},
   "source": [
    "# producing graphs\n",
    "Add the ollama results for llama3 8b and change below!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d58aa5-af49-46d3-a98c-2beda6014e60",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary_ = summary[summary['model'] != 'meta/meta-llama-3-8b-instruct']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4fa472-d761-4712-8e71-f4dd50175efb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = summary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e999fd0-682e-4073-a4ad-cd1fbe2a1511",
   "metadata": {},
   "source": [
    "## reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c045dc-d8a7-4a90-aa1a-7fbc4849ea64",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiments_1 = [\n",
    "    'Basic zero-shot prompting', 'lets think step by step',\n",
    "    'zero-shot cot with alex template', 'zero-shot cot with legal template'\n",
    "]\n",
    "\n",
    "filtered_df = df[df['experiment_name'].isin(experiments_1)]\n",
    "pivot_df = filtered_df.pivot(index='experiment_name', columns='model', values='F1_new')\n",
    "\n",
    "new_labels = {\n",
    "    'Basic zero-shot prompting': 'basic zero-shot',\n",
    "    'lets think step by step': \"let's think step by step\",\n",
    "    'zero-shot cot with alex template': 'zero-shot with legal template A',\n",
    "    'zero-shot cot with legal template': 'zero-shot with legal template B',\n",
    "}\n",
    "\n",
    "\n",
    "model_name_map = {\n",
    "    'claude-3-5-sonnet-20240620': 'Claude-3.5 Sonnet',\n",
    "    'claude-3-opus-20240229': 'Claude-3 Opus',\n",
    "    'gpt-3.5-turbo-0125': 'GPT-3.5 Turbo',\n",
    "    'gpt-4-turbo': 'GPT-4 Turbo',\n",
    "    'gpt-4': 'GPT-4',\n",
    "    'gpt-4o': 'GPT-4o',\n",
    "    'meta/meta-llama-3-70b-instruct': 'Lllama-3 70B',\n",
    "    #'meta/meta-llama-3-8b-instruct' : 'Llama-3 8B',\n",
    "    'llama3:8b': 'LLama-3 8B'\n",
    "}\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "# Define bar width and positions\n",
    "bar_width = 0.09\n",
    "index = np.arange(len(pivot_df))\n",
    "\n",
    "# Create bars for each model\n",
    "bars = []\n",
    "models = list(model_name_map.keys())\n",
    "colors = ['teal', 'steelblue', 'darkseagreen', 'olive', 'yellow', 'yellowgreen', 'mediumpurple','indigo']\n",
    "#hatches = ['-', '', '', '//', '', '/', '', '|']\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    #bars.append(ax.bar(index + i * bar_width, pivot_df[model], bar_width, label=model_name_map[model], color=colors[i], hatch=hatches[i]))\n",
    "    bars.append(ax.bar(index + i * bar_width, pivot_df[model], bar_width, label=model_name_map[model], color=colors[i]))\n",
    "\n",
    "# Add F1 score values on the bars\n",
    "for i in range(len(pivot_df)):\n",
    "    for j, model in enumerate(models):\n",
    "        if not pd.isna(pivot_df[model].iloc[i]):\n",
    "            ax.text(index[i] + j * bar_width, pivot_df[model].iloc[i] + 0.001, f'{pivot_df[model].iloc[i]:.2f}', ha='center', va='bottom')\n",
    "\n",
    "# Add labels, title, and legend\n",
    "ax.set_xlabel('Prompt Engineering Method')\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.set_xticks(index + (len(models) / 2 - 0.5) * bar_width)\n",
    "ax.set_xticklabels([new_labels[name] for name in pivot_df.index], rotation=10, ha='right')\n",
    "\n",
    "# Move legend to the right outside of the plot\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Adjust layout to make room for the rotated labels and legend\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('basic ones', format = 'pdf')\n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c979f890-644c-49fc-aec1-2c652c236dee",
   "metadata": {},
   "source": [
    "## annollm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71bff57-046f-469a-a813-4f65ff85d799",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiments_2 = [\n",
    "    'Basic zero-shot prompting',\n",
    "    #'annollm with 1 example per class, examples inside prompt',\n",
    "    'annollm with 1 example per class, examples inside system message',\n",
    "    #'annollm with 2 examples per class, examples inside prompt',\n",
    "    'annollm with 2 examples per class, examples inside system message',\n",
    "    #'annollm with 3 examples per class, examples inside prompt',\n",
    "    'annollm with 3 examples per class, examples inside system message'\n",
    "]\n",
    "\n",
    "filtered_df = df[df['experiment_name'].isin(experiments_2)]\n",
    "pivot_df = filtered_df.pivot(index='experiment_name', columns='model', values='F1_new')\n",
    "\n",
    "new_labels = {\n",
    "    'Basic zero-shot prompting': 'basic zero-shot',\n",
    "    #'annollm with 1 example per class, examples inside prompt': '1 example per class in the prompt',\n",
    "    'annollm with 1 example per class, examples inside system message': '1 example per class',\n",
    "    #'annollm with 2 examples per class, examples inside prompt': '2 examples per class in the prompt',\n",
    "    'annollm with 2 examples per class, examples inside system message': '2 examples per class',\n",
    "    #'annollm with 3 examples per class, examples inside prompt': '3 examples per class in the prompt',\n",
    "    'annollm with 3 examples per class, examples inside system message': '3 examples per class'\n",
    "}\n",
    "\n",
    "model_name_map = {\n",
    "    'claude-3-5-sonnet-20240620': 'Claude-3.5 Sonnet',\n",
    "    'claude-3-opus-20240229': 'Claude-3 Opus',\n",
    "    'gpt-3.5-turbo-0125': 'GPT-3.5 Turbo',\n",
    "    'gpt-4-turbo': 'GPT-4 Turbo',\n",
    "    'gpt-4': 'GPT-4',\n",
    "    'gpt-4o': 'GPT-4o',\n",
    "    'meta/meta-llama-3-70b-instruct': 'Lllama-3 70B',\n",
    "    'llama3:8b': 'Llama-3 8B'\n",
    "}\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "# Define bar width and positions\n",
    "bar_width = 0.09\n",
    "index = np.arange(len(pivot_df))\n",
    "\n",
    "# Create bars for each model\n",
    "bars = []\n",
    "models = list(model_name_map.keys())\n",
    "colors = ['teal', 'steelblue', 'darkseagreen', 'olive', 'yellow', 'yellowgreen', 'mediumpurple','indigo']\n",
    "#hatches = ['-', '', '', '//', '', '/', '', '|']\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    bars.append(ax.bar(index + i * bar_width, pivot_df[model], bar_width, label=model_name_map[model], color=colors[i]))\n",
    "\n",
    "# Add F1 score values on the bars\n",
    "for i in range(len(pivot_df)):\n",
    "    for j, model in enumerate(models):\n",
    "        if not pd.isna(pivot_df[model].iloc[i]):\n",
    "            ax.text(index[i] + j * bar_width, pivot_df[model].iloc[i] + 0.001, f'{pivot_df[model].iloc[i]:.2f}', ha='center', va='bottom')\n",
    "\n",
    "# Add labels, title, and legend\n",
    "ax.set_xlabel('Number of In-context Examples')\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.set_title('AnnoLLM, Examples in System Message')\n",
    "ax.set_xticks(index + (len(models) / 2 - 0.5) * bar_width)\n",
    "ax.set_xticklabels([new_labels[name] for name in pivot_df.index], rotation=10, ha='right')\n",
    "ax.set_ylim(0, 0.75)\n",
    "\n",
    "# Move legend to the right outside of the plot\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Adjust layout to make room for the rotated labels and legend\n",
    "plt.tight_layout()\n",
    "plt.savefig('annollm in system message.pdf', format = 'pdf')\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf415d7e-ec53-415e-99cb-5435bda3218f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiments_2 = [\n",
    "    'Basic zero-shot prompting',\n",
    "    'annollm with 1 example per class, examples inside prompt',\n",
    "    #'annollm with 1 example per class, examples inside system message',\n",
    "    'annollm with 2 examples per class, examples inside prompt',\n",
    "    #'annollm with 2 examples per class, examples inside system message',\n",
    "    'annollm with 3 examples per class, examples inside prompt',\n",
    "    #'annollm with 3 examples per class, examples inside system message'\n",
    "]\n",
    "\n",
    "filtered_df = df[df['experiment_name'].isin(experiments_2)]\n",
    "pivot_df = filtered_df.pivot(index='experiment_name', columns='model', values='F1_new')\n",
    "\n",
    "new_labels = {\n",
    "    'Basic zero-shot prompting': 'basic zero-shot',\n",
    "    'annollm with 1 example per class, examples inside prompt': '1 example per class',\n",
    "    #'annollm with 1 example per class, examples inside system message': '1 example per class in the system message',\n",
    "    'annollm with 2 examples per class, examples inside prompt': '2 examples per class',\n",
    "    #'annollm with 2 examples per class, examples inside system message': '2 examples per class in the system message',\n",
    "    'annollm with 3 examples per class, examples inside prompt': '3 examples per class',\n",
    "    #'annollm with 3 examples per class, examples inside system message': '3 examples per class in the system message'\n",
    "}\n",
    "\n",
    "model_name_map = {\n",
    "    'claude-3-5-sonnet-20240620': 'Claude-3.5 Sonnet',\n",
    "    'claude-3-opus-20240229': 'Claude-3 Opus',\n",
    "    'gpt-3.5-turbo-0125': 'GPT-3.5 Turbo',\n",
    "    'gpt-4-turbo': 'GPT-4 Turbo',\n",
    "    'gpt-4': 'GPT-4',\n",
    "    'gpt-4o': 'GPT-4o',\n",
    "    'meta/meta-llama-3-70b-instruct': 'Lllama-3 70B',\n",
    "    'llama3:8b': 'Llama-3 8B'\n",
    "}\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "# Define bar width and positions\n",
    "bar_width = 0.09\n",
    "index = np.arange(len(pivot_df))\n",
    "\n",
    "# Create bars for each model\n",
    "bars = []\n",
    "models = list(model_name_map.keys())\n",
    "colors = ['teal', 'steelblue', 'darkseagreen', 'olive', 'yellow', 'yellowgreen', 'mediumpurple','indigo']\n",
    "#hatches = ['-', '', '', '//', '', '/', '', '|']\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    bars.append(ax.bar(index + i * bar_width, pivot_df[model], bar_width, label=model_name_map[model], color=colors[i]))\n",
    "\n",
    "# Add F1 score values on the bars\n",
    "for i in range(len(pivot_df)):\n",
    "    for j, model in enumerate(models):\n",
    "        if not pd.isna(pivot_df[model].iloc[i]):\n",
    "            ax.text(index[i] + j * bar_width, pivot_df[model].iloc[i] + 0.001, f'{pivot_df[model].iloc[i]:.2f}', ha='center', va='bottom')\n",
    "\n",
    "# Add labels, title, and legend\n",
    "ax.set_xlabel('Number of In-context Examples')\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.set_title('AnnoLLM, Examples in User Message')\n",
    "ax.set_xticks(index + (len(models) / 2 - 0.5) * bar_width)\n",
    "ax.set_xticklabels([new_labels[name] for name in pivot_df.index], rotation=10, ha='right')\n",
    "\n",
    "ax.set_ylim(0, 0.75)\n",
    "\n",
    "# Move legend to the right outside of the plot\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Adjust layout to make room for the rotated labels and legend\n",
    "plt.tight_layout()\n",
    "plt.savefig('annollm in user message.pdf', format = 'pdf')\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4651167-0fb7-45ec-a7df-fbb7cd38ee1a",
   "metadata": {},
   "source": [
    "## few-shot without embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68a4236-1adc-4c62-b352-7646914fd92a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiments_3 = [\n",
    "    'few-shot with 1 example per class',\n",
    "    'few-shot with 1 example per class, examples in prompt',\n",
    "    'few-shot with 2 examples per class',\n",
    "    'few-shot with 2 examples per class, examples in prompt',\n",
    "    'few-shot with 3 examples per class',\n",
    "    'few-shot with 3 examples per class, examples in prompt'\n",
    "]\n",
    "\n",
    "filtered_df = df[df['experiment_name'].isin(experiments_3)]\n",
    "pivot_df = filtered_df.pivot(index='experiment_name', columns='model', values='F1_new')\n",
    "\n",
    "new_labels = {\n",
    "    'few-shot with 1 example per class': '1 example per class in the system message',\n",
    "    'few-shot with 1 example per class, examples in prompt': '1 example per class in the prompt',\n",
    "    'few-shot with 2 examples per class': '2 examples per class in the system message',\n",
    "    'few-shot with 2 examples per class, examples in prompt': '2 examples per class in the prompt',\n",
    "    'few-shot with 3 examples per class': '3 examples per class in the system message',\n",
    "    'few-shot with 3 examples per class, examples in prompt': '3 examples per class in the prompt'\n",
    "}\n",
    "\n",
    "model_name_map = {\n",
    "    'claude-3-5-sonnet-20240620': 'Claude-3.5 Sonnet',\n",
    "    'claude-3-opus-20240229': 'Claude-3 Opus',\n",
    "    'gpt-3.5-turbo-0125': 'GPT-3.5 Turbo',\n",
    "    'gpt-4-turbo': 'GPT-4 Turbo',\n",
    "    'gpt-4': 'GPT-4',\n",
    "    'gpt-4o': 'GPT-4o',\n",
    "    'meta/meta-llama-3-70b-instruct': 'Lllama-3 70B',\n",
    "    #'meta/meta-llama-3-8b-instruct': 'Llama-3 8B'\n",
    "    'llama3:8b': 'LLama-3 8B'\n",
    "}\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "# Define bar width and positions\n",
    "bar_width = 0.09\n",
    "index = np.arange(len(pivot_df))\n",
    "\n",
    "# Create bars for each model\n",
    "bars = []\n",
    "models = list(model_name_map.keys())\n",
    "colors = ['teal', 'steelblue', 'darkseagreen', 'olive', 'yellow', 'yellowgreen', 'mediumpurple','indigo']\n",
    "#hatches = ['-', '', '', '//', '', '/', '', '|']\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    bars.append(ax.bar(index + i * bar_width, pivot_df[model], bar_width, label=model_name_map[model], color=colors[i]))\n",
    "\n",
    "# Add F1 score values on the bars\n",
    "for i in range(len(pivot_df)):\n",
    "    for j, model in enumerate(models):\n",
    "        if not pd.isna(pivot_df[model].iloc[i]):\n",
    "            ax.text(index[i] + j * bar_width, pivot_df[model].iloc[i] + 0.001, f'{pivot_df[model].iloc[i]:.2f}', ha='center', va='bottom')\n",
    "\n",
    "# Add labels, title, and legend\n",
    "ax.set_xlabel('Prompt Engineering Method')\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.set_title('Few-Shot without Embeddings')\n",
    "ax.set_xticks(index + (len(models) / 2 - 0.5) * bar_width)\n",
    "ax.set_xticklabels([new_labels[name] for name in pivot_df.index], rotation=10, ha='right')\n",
    "\n",
    "ax.set_ylim(0, 0.75)\n",
    "\n",
    "# Move legend to the right outside of the plot\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Adjust layout to make room for the rotated labels and legend\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd18d59-2e7f-4b69-99eb-3335e57c4eff",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiments_3 = [\n",
    "    'Basic zero-shot prompting',\n",
    "    #'few-shot with 1 example per class',\n",
    "    'few-shot with 1 example per class, examples in prompt',\n",
    "    #'few-shot with 2 examples per class',\n",
    "    'few-shot with 2 examples per class, examples in prompt',\n",
    "    #'few-shot with 3 examples per class',\n",
    "    'few-shot with 3 examples per class, examples in prompt'\n",
    "]\n",
    "\n",
    "filtered_df = df[df['experiment_name'].isin(experiments_3)]\n",
    "pivot_df = filtered_df.pivot(index='experiment_name', columns='model', values='F1_new')\n",
    "\n",
    "new_labels = {\n",
    "    'Basic zero-shot prompting': 'basic zero-shot',\n",
    "    #'few-shot with 1 example per class': '1 example per class in the system message',\n",
    "    'few-shot with 1 example per class, examples in prompt': '1 example per class',\n",
    "    #'few-shot with 2 examples per class': '2 examples per class in the system message',\n",
    "    'few-shot with 2 examples per class, examples in prompt': '2 examples per class',\n",
    "    #'few-shot with 3 examples per class': '3 examples per class in the system message',\n",
    "    'few-shot with 3 examples per class, examples in prompt': '3 examples per class'\n",
    "}\n",
    "\n",
    "model_name_map = {\n",
    "    'claude-3-5-sonnet-20240620': 'Claude-3.5 Sonnet',\n",
    "    'claude-3-opus-20240229': 'Claude-3 Opus',\n",
    "    'gpt-3.5-turbo-0125': 'GPT-3.5 Turbo',\n",
    "    'gpt-4-turbo': 'GPT-4 Turbo',\n",
    "    'gpt-4': 'GPT-4',\n",
    "    'gpt-4o': 'GPT-4o',\n",
    "    'meta/meta-llama-3-70b-instruct': 'Lllama-3 70B',\n",
    "    #'meta/meta-llama-3-8b-instruct': 'Llama-3 8B'\n",
    "    'llama3:8b': 'LLama-3 8B'\n",
    "}\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "# Define bar width and positions\n",
    "bar_width = 0.09\n",
    "index = np.arange(len(pivot_df))\n",
    "\n",
    "# Create bars for each model\n",
    "bars = []\n",
    "models = list(model_name_map.keys())\n",
    "colors = ['teal', 'steelblue', 'darkseagreen', 'olive', 'yellow', 'yellowgreen', 'mediumpurple','indigo']\n",
    "#hatches = ['-', '', '', '//', '', '/', '', '|']\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    bars.append(ax.bar(index + i * bar_width, pivot_df[model], bar_width, label=model_name_map[model], color=colors[i]))\n",
    "\n",
    "# Add F1 score values on the bars\n",
    "for i in range(len(pivot_df)):\n",
    "    for j, model in enumerate(models):\n",
    "        if not pd.isna(pivot_df[model].iloc[i]):\n",
    "            ax.text(index[i] + j * bar_width, pivot_df[model].iloc[i] + 0.001, f'{pivot_df[model].iloc[i]:.2f}', ha='center', va='bottom')\n",
    "\n",
    "# Add labels, title, and legend\n",
    "ax.set_xlabel('Number of In-context Examples')\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.set_title('Simple Few-Shot Prompting, Examples in User Message')\n",
    "ax.set_xticks(index + (len(models) / 2 - 0.5) * bar_width)\n",
    "ax.set_xticklabels([new_labels[name] for name in pivot_df.index], rotation=10, ha='right')\n",
    "\n",
    "ax.set_ylim(0, 0.75)\n",
    "\n",
    "# Move legend to the right outside of the plot\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Adjust layout to make room for the rotated labels and legend\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('simple fewshot in user message.pdf', format = 'pdf')\n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd13ec5-c946-4f5e-aff3-8b21830e1464",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiments_3 = [\n",
    "    'Basic zero-shot prompting',\n",
    "    'few-shot with 1 example per class',\n",
    "    #'few-shot with 1 example per class, examples in prompt',\n",
    "    'few-shot with 2 examples per class',\n",
    "    #'few-shot with 2 examples per class, examples in prompt',\n",
    "    'few-shot with 3 examples per class',\n",
    "    #'few-shot with 3 examples per class, examples in prompt'\n",
    "]\n",
    "\n",
    "filtered_df = df[df['experiment_name'].isin(experiments_3)]\n",
    "pivot_df = filtered_df.pivot(index='experiment_name', columns='model', values='F1_new')\n",
    "\n",
    "new_labels = {\n",
    "    'Basic zero-shot prompting': 'basic zero-shot',\n",
    "    'few-shot with 1 example per class': '1 example per class',\n",
    "    #'few-shot with 1 example per class, examples in prompt': '1 example per class',\n",
    "    'few-shot with 2 examples per class': '2 examples per class',\n",
    "    #'few-shot with 2 examples per class, examples in prompt': '2 examples per class',\n",
    "    'few-shot with 3 examples per class': '3 examples per class',\n",
    "    #'few-shot with 3 examples per class, examples in prompt': '3 examples per class'\n",
    "}\n",
    "\n",
    "model_name_map = {\n",
    "    'claude-3-5-sonnet-20240620': 'Claude-3.5 Sonnet',\n",
    "    'claude-3-opus-20240229': 'Claude-3 Opus',\n",
    "    'gpt-3.5-turbo-0125': 'GPT-3.5 Turbo',\n",
    "    'gpt-4-turbo': 'GPT-4 Turbo',\n",
    "    'gpt-4': 'GPT-4',\n",
    "    'gpt-4o': 'GPT-4o',\n",
    "    'meta/meta-llama-3-70b-instruct': 'Lllama-3 70B',\n",
    "    #'meta/meta-llama-3-8b-instruct': 'Llama-3 8B'\n",
    "    'llama3:8b': 'LLama-3 8B'\n",
    "}\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "# Define bar width and positions\n",
    "bar_width = 0.09\n",
    "index = np.arange(len(pivot_df))\n",
    "\n",
    "# Create bars for each model\n",
    "bars = []\n",
    "models = list(model_name_map.keys())\n",
    "colors = ['teal', 'steelblue', 'darkseagreen', 'olive', 'yellow', 'yellowgreen', 'mediumpurple','indigo']\n",
    "#hatches = ['-', '', '', '//', '', '/', '', '|']\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    bars.append(ax.bar(index + i * bar_width, pivot_df[model], bar_width, label=model_name_map[model], color=colors[i]))\n",
    "\n",
    "# Add F1 score values on the bars\n",
    "for i in range(len(pivot_df)):\n",
    "    for j, model in enumerate(models):\n",
    "        if not pd.isna(pivot_df[model].iloc[i]):\n",
    "            ax.text(index[i] + j * bar_width, pivot_df[model].iloc[i] + 0.001, f'{pivot_df[model].iloc[i]:.2f}', ha='center', va='bottom')\n",
    "\n",
    "# Add labels, title, and legend\n",
    "ax.set_xlabel('Number of In-context Examples')\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.set_title('Simple Few-Shot Prompting, Examples in System Message')\n",
    "ax.set_xticks(index + (len(models) / 2 - 0.5) * bar_width)\n",
    "ax.set_xticklabels([new_labels[name] for name in pivot_df.index], rotation=10, ha='right')\n",
    "\n",
    "ax.set_ylim(0, 0.75)\n",
    "\n",
    "# Move legend to the right outside of the plot\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Adjust layout to make room for the rotated labels and legend\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('simple fewshot in system message.pdf', format = 'pdf')\n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18018e0-6420-4a6d-bb5e-dd5e7265251a",
   "metadata": {},
   "source": [
    "## few-shot with embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebf945d-6702-4108-bc24-4c1747ebdd47",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiments_4 = [\n",
    "    'few shot with embeddings not per class, two examples in total, in prompt',\n",
    "    'few shot with embeddings not per class, two examples in total, in system message',\n",
    "    'few shot with embeddings not per class, four examples in total, in prompt',\n",
    "    'few shot with embeddings not per class, four examples in total, in system message',\n",
    "    'few shot with embeddings not per class, six examples in total, in prompt',\n",
    "    'few shot with embeddings not per class, six examples in total, in system message',\n",
    "    'few shot with embeddings, per class, one example per class, in prompt',\n",
    "    'few shot with embeddings, per class, one example per class, in system message',\n",
    "    'few shot with embeddings, per class, two examples per class, in prompt',\n",
    "    'few shot with embeddings, per class, two examples per class, in system message',\n",
    "    'few shot with embeddings, per class, three examples per class, in prompt',\n",
    "    'few shot with embeddings, per class, three examples per class, in system message'\n",
    "]\n",
    "\n",
    "filtered_df = df[df['experiment_name'].isin(experiments_4)]\n",
    "pivot_df = filtered_df.pivot(index='experiment_name', columns='model', values='F1_new')\n",
    "\n",
    "new_labels = {\n",
    "    'few shot with embeddings not per class, two examples in total, in prompt': '2 in total in prompt',\n",
    "    'few shot with embeddings not per class, two examples in total, in system message': '2 in total in system message',\n",
    "    'few shot with embeddings not per class, four examples in total, in prompt': '4 in total in prompt',\n",
    "    'few shot with embeddings not per class, four examples in total, in system message': '4 in total in system message',\n",
    "    'few shot with embeddings not per class, six examples in total, in prompt': '6 in total in prompt',\n",
    "    'few shot with embeddings not per class, six examples in total, in system message': '6 in total in system message',\n",
    "    'few shot with embeddings, per class, one example per class, in prompt': '1 per class in prompt',\n",
    "    'few shot with embeddings, per class, one example per class, in system message': '1 per class in system message',\n",
    "    'few shot with embeddings, per class, two examples per class, in prompt': '2 per class in prompt',\n",
    "    'few shot with embeddings, per class, two examples per class, in system message': '2 per class in system message',\n",
    "    'few shot with embeddings, per class, three examples per class, in prompt': '3 per class in prompt',\n",
    "    'few shot with embeddings, per class, three examples per class, in system message': '3 per class in system message'\n",
    "}\n",
    "\n",
    "model_name_map = {\n",
    "    'claude-3-5-sonnet-20240620': 'Claude-3.5 Sonnet',\n",
    "    'claude-3-opus-20240229': 'Claude-3 Opus',\n",
    "    'gpt-3.5-turbo-0125': 'GPT-3.5 Turbo',\n",
    "    'gpt-4-turbo': 'GPT-4 Turbo',\n",
    "    'gpt-4': 'GPT-4',\n",
    "    'gpt-4o': 'GPT-4o',\n",
    "    'meta/meta-llama-3-70b-instruct': 'Llama-3 70B',\n",
    "    'meta/meta-llama-3-8b-instruct': 'Llama-3 8B'\n",
    "}\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "# Define bar width and positions\n",
    "bar_width = 0.09\n",
    "index = np.arange(len(pivot_df))\n",
    "\n",
    "# Create bars for each model\n",
    "bars = []\n",
    "models = list(model_name_map.keys())\n",
    "colors = ['darkseagreen', 'gold', 'tab:olive', 'olivedrab', 'khaki', 'darkolivegreen', 'darkkhaki', 'olive']\n",
    "hatches = ['-', '', '', '//', '', '/', '', '|']\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    bars.append(ax.bar(index + i * bar_width, pivot_df[model], bar_width, label=model_name_map[model], color=colors[i], hatch=hatches[i]))\n",
    "\n",
    "# Add F1 score values on the bars\n",
    "for i in range(len(pivot_df)):\n",
    "    for j, model in enumerate(models):\n",
    "        if not pd.isna(pivot_df[model].iloc[i]):\n",
    "            ax.text(index[i] + j * bar_width, pivot_df[model].iloc[i] + 0.001, f'{pivot_df[model].iloc[i]:.2f}', ha='center', va='bottom')\n",
    "\n",
    "# Add labels, title, and legend\n",
    "ax.set_xlabel('Prompt Engineering Method')\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.set_title('Few-Shot with Embeddings')\n",
    "ax.set_xticks(index + (len(models) / 2 - 0.5) * bar_width)\n",
    "ax.set_xticklabels([new_labels[name] for name in pivot_df.index], rotation=10, ha='right')\n",
    "\n",
    "ax.set_ylim(0, 0.90)\n",
    "\n",
    "# Move legend to the right outside of the plot\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Adjust layout to make room for the rotated labels and legend\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the chart\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02949ba-cce9-41fd-a620-da29a74e02ea",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiments_4 = [\n",
    "    'Basic zero-shot prompting',\n",
    "    'few shot with embeddings, per class, one example per class, in prompt',\n",
    "    'few shot with embeddings, per class, two examples per class, in prompt',\n",
    "    'few shot with embeddings, per class, three examples per class, in prompt',\n",
    "]\n",
    "\n",
    "filtered_df = df[df['experiment_name'].isin(experiments_4)].copy()\n",
    "filtered_df.loc[:, 'experiment_name'] = pd.Categorical(filtered_df['experiment_name'], categories=experiments_4, ordered=True)\n",
    "filtered_df = filtered_df.sort_values('experiment_name')\n",
    "pivot_df = filtered_df.pivot(index='experiment_name', columns='model', values='F1_new')\n",
    "\n",
    "new_labels = {\n",
    "    'Basic zero-shot prompting': 'basic zero-shot',\n",
    "    'few shot with embeddings, per class, one example per class, in prompt': '1 per class',\n",
    "    'few shot with embeddings, per class, two examples per class, in prompt': '2 per class',\n",
    "    'few shot with embeddings, per class, three examples per class, in prompt': '3 per class'\n",
    "}\n",
    "\n",
    "model_name_map = {\n",
    "    'claude-3-5-sonnet-20240620': 'Claude-3.5 Sonnet',\n",
    "    'claude-3-opus-20240229': 'Claude-3 Opus',\n",
    "    'gpt-3.5-turbo-0125': 'GPT-3.5 Turbo',\n",
    "    'gpt-4-turbo': 'GPT-4 Turbo',\n",
    "    'gpt-4': 'GPT-4',\n",
    "    'gpt-4o': 'GPT-4o',\n",
    "    'meta/meta-llama-3-70b-instruct': 'Llama-3 70B',\n",
    "    'llama3:8b': 'Llama-3 8B'\n",
    "}\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "# Define bar width and positions\n",
    "bar_width = 0.09\n",
    "index = np.arange(len(pivot_df))\n",
    "\n",
    "# Create bars for each model\n",
    "bars = []\n",
    "models = list(model_name_map.keys())\n",
    "colors = ['teal', 'steelblue', 'darkseagreen', 'olive', 'yellow', 'yellowgreen', 'mediumpurple','indigo']\n",
    "#hatches = ['-', '', '', '//', '', '/', '', '|']\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    bars.append(ax.bar(index + i * bar_width, pivot_df[model], bar_width, label=model_name_map[model], color=colors[i]))\n",
    "\n",
    "# Add F1 score values on the bars\n",
    "for i in range(len(pivot_df)):\n",
    "    for j, model in enumerate(models):\n",
    "        if not pd.isna(pivot_df[model].iloc[i]):\n",
    "            ax.text(index[i] + j * bar_width, pivot_df[model].iloc[i] + 0.001, f'{pivot_df[model].iloc[i]:.2f}', ha='center', va='bottom')\n",
    "\n",
    "# Add labels, title, and legend\n",
    "ax.set_xlabel('Number of In-context Examples')\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.set_title('Few-Shot Prompting with Embeddings, Examples in User Message')\n",
    "ax.set_xticks(index + (len(models) / 2 - 0.5) * bar_width)\n",
    "ax.set_xticklabels([new_labels[name] for name in pivot_df.index], rotation=10, ha='right')\n",
    "\n",
    "ax.set_ylim(0, 0.90)\n",
    "\n",
    "# Move legend to the right outside of the plot\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Adjust layout to make room for the rotated labels and legend\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('few shot with embeddings in user message.pdf', format = 'pdf')\n",
    "# Display the chart\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bc85b6-f9a1-47f4-9035-6193b5f361d1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiments_4 = [\n",
    "    'Basic zero-shot prompting',\n",
    "    'few shot with embeddings, per class, one example per class, in system message',\n",
    "    'few shot with embeddings, per class, two examples per class, in system message',\n",
    "    'few shot with embeddings, per class, three examples per class, in system message'\n",
    "]\n",
    "\n",
    "filtered_df = df[df['experiment_name'].isin(experiments_4)].copy()\n",
    "filtered_df['experiment_name'] = pd.Categorical(filtered_df['experiment_name'], categories=experiments_4, ordered=True)\n",
    "filtered_df = filtered_df.sort_values('experiment_name')\n",
    "pivot_df = filtered_df.pivot(index='experiment_name', columns='model', values='F1_new')\n",
    "\n",
    "new_labels = {\n",
    "    'Basic zero-shot prompting': 'basic zero-shot',\n",
    "    'few shot with embeddings, per class, one example per class, in system message': '1 per class',\n",
    "    'few shot with embeddings, per class, two examples per class, in system message': '2 per class',\n",
    "    'few shot with embeddings, per class, three examples per class, in system message': '3 per class'\n",
    "}\n",
    "\n",
    "model_name_map = {\n",
    "    'claude-3-5-sonnet-20240620': 'Claude-3.5 Sonnet',\n",
    "    'claude-3-opus-20240229': 'Claude-3 Opus',\n",
    "    'gpt-3.5-turbo-0125': 'GPT-3.5 Turbo',\n",
    "    'gpt-4-turbo': 'GPT-4 Turbo',\n",
    "    'gpt-4': 'GPT-4',\n",
    "    'gpt-4o': 'GPT-4o',\n",
    "    'meta/meta-llama-3-70b-instruct': 'Llama-3 70B',\n",
    "    'llama3:8b': 'Llama-3 8B'\n",
    "}\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "# Define bar width and positions\n",
    "bar_width = 0.09\n",
    "index = np.arange(len(pivot_df))\n",
    "\n",
    "# Create bars for each model\n",
    "bars = []\n",
    "models = list(model_name_map.keys())\n",
    "colors = ['teal', 'steelblue', 'darkseagreen', 'olive', 'yellow', 'yellowgreen', 'mediumpurple','indigo']\n",
    "#hatches = ['-', '', '', '//', '', '/', '', '|']\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    bars.append(ax.bar(index + i * bar_width, pivot_df[model], bar_width, label=model_name_map[model], color=colors[i]))\n",
    "\n",
    "# Add F1 score values on the bars\n",
    "for i in range(len(pivot_df)):\n",
    "    for j, model in enumerate(models):\n",
    "        if not pd.isna(pivot_df[model].iloc[i]):\n",
    "            ax.text(index[i] + j * bar_width, pivot_df[model].iloc[i] + 0.001, f'{pivot_df[model].iloc[i]:.2f}', ha='center', va='bottom')\n",
    "\n",
    "# Add labels, title, and legend\n",
    "ax.set_xlabel('Number of In-context Examples')\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.set_title('Few-Shot Prompting with Embeddings, Examples in System Message')\n",
    "ax.set_xticks(index + (len(models) / 2 - 0.5) * bar_width)\n",
    "ax.set_xticklabels([new_labels[name] for name in pivot_df.index], rotation=10, ha='right')\n",
    "\n",
    "ax.set_ylim(0, 0.90)\n",
    "\n",
    "# Move legend to the right outside of the plot\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Adjust layout to make room for the rotated labels and legend\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('few shot with embeddings in system message.pdf', format = 'pdf')\n",
    "\n",
    "# Display the chart\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45701faf-7bd3-4872-bfeb-8ad24a8ebe5b",
   "metadata": {},
   "source": [
    "## appending the contract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bef0c4b-8be8-493d-96a6-bdaab3c6a457",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiments_4 = [\n",
    "    'Basic zero-shot prompting',\n",
    "    'appending the whole contract in the system message',\n",
    "    'appending the whole contract to the prompt'\n",
    "]\n",
    "\n",
    "filtered_df = df[df['experiment_name'].isin(experiments_4)].copy()\n",
    "filtered_df.loc[:, 'experiment_name'] = pd.Categorical(filtered_df['experiment_name'], categories=experiments_4, ordered=True)\n",
    "filtered_df = filtered_df.sort_values('experiment_name')\n",
    "pivot_df = filtered_df.pivot(index='experiment_name', columns='model', values='F1_new')\n",
    "\n",
    "new_labels = {\n",
    "    'Basic zero-shot prompting': 'basic zero-shot',\n",
    "    'appending the whole contract in the system message': 'contract in the system message',\n",
    "    'appending the whole contract to the prompt': 'contract in the user message'\n",
    "}\n",
    "\n",
    "model_name_map = {\n",
    "    'claude-3-5-sonnet-20240620': 'Claude-3.5 Sonnet',\n",
    "    'claude-3-opus-20240229': 'Claude-3 Opus',\n",
    "    #'gpt-3.5-turbo-0125': 'GPT-3.5 Turbo',\n",
    "    'gpt-4-turbo': 'GPT-4 Turbo',\n",
    "    #'gpt-4': 'GPT-4',\n",
    "    'gpt-4o': 'GPT-4o',\n",
    "    #'meta/meta-llama-3-70b-instruct': 'Llama-3 70B',\n",
    "    #'llama3:8b': 'Llama-3 8B'\n",
    "}\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "# Define bar width and positions\n",
    "bar_width = 0.09\n",
    "index = np.arange(len(pivot_df))\n",
    "\n",
    "# Create bars for each model\n",
    "bars = []\n",
    "models = list(model_name_map.keys())\n",
    "colors = ['teal', 'steelblue', 'darkseagreen', 'olive', 'yellow', 'yellowgreen', 'mediumpurple','indigo']\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    bars.append(ax.bar(index + i * bar_width, pivot_df[model], bar_width, label=model_name_map[model], color=colors[i]))\n",
    "\n",
    "# Add F1 score values on the bars\n",
    "for i in range(len(pivot_df)):\n",
    "    for j, model in enumerate(models):\n",
    "        if not pd.isna(pivot_df[model].iloc[i]):\n",
    "            ax.text(index[i] + j * bar_width, pivot_df[model].iloc[i] + 0.001, f'{pivot_df[model].iloc[i]:.2f}', ha='center', va='bottom')\n",
    "\n",
    "# Add labels, title, and legend\n",
    "ax.set_xlabel('Prompt Engineering Method')\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.set_title('Providing the Source Contract to the LLM')\n",
    "ax.set_xticks(index + (len(models) / 2 - 0.5) * bar_width)\n",
    "ax.set_xticklabels([new_labels[name] for name in pivot_df.index], rotation=10, ha='right')\n",
    "\n",
    "#ax.set_ylim(0, 0.90)\n",
    "\n",
    "# Move legend to the right outside of the plot\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Adjust layout to make room for the rotated labels and legend\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('providing the source contract.pdf', format = 'pdf')\n",
    "# Display the chart\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd5d322-02eb-491c-bf3b-28075b3ba9df",
   "metadata": {},
   "source": [
    "## cost graphs for GPT family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629a42f4-c4c4-4a97-9aef-02b7c4acd20a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpt_models = ['gpt-3.5-turbo-0125', 'gpt-4-turbo', 'gpt-4', 'gpt-4o']\n",
    "columns_to_consider = ['experiment_name', 'model', 'F1_new','total cost of this experiment','experiment ran for (seconds)']\n",
    "#columns_to_consider = ['experiment_name', 'model', 'F1_new','experiment ran for (seconds)']\n",
    "unique_values = {col: [] for col in columns_to_consider}\n",
    "\n",
    "# Iterate through each DataFrame and collect unique values\n",
    "for df in updated_results:\n",
    "    if df['model'][0] in ['gpt-3.5-turbo-0125', 'gpt-4-turbo', 'gpt-4', 'gpt-4o']:\n",
    "        df_gpt = df[df['model'].isin(gpt_models)]\n",
    "        for col in columns_to_consider:\n",
    "            unique_val = df_gpt[col].unique()\n",
    "            if len(unique_val) == 1:  # Ensure it's a unique value column\n",
    "                unique_values[col].append(unique_val[0])\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "summary_gpt_subset = pd.DataFrame(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6470eb6-aece-4cac-a895-a2da411422b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary_gpt_subset['group'] = summary_gpt_subset.index // 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7042b8d4-dbf3-416a-9532-29c4c57e2594",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_grouped = summary_gpt_subset.groupby('group')['F1_new'].mean().reset_index()\n",
    "\n",
    "df_grouped.columns = ['group', 'avg F1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5d7b17-fbca-4458-a3c6-a948e5a6044d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = gpt_models\n",
    "\n",
    "# Example list of chosen experiment types\n",
    "chosen_experiments = ['Basic zero-shot prompting', 'zero-shot cot with legal template', 'lets think step by step', 'annollm with 2 examples per class, examples inside prompt',\\\n",
    "                      'few shot with embeddings, per class, three examples per class, in system message', 'few-shot with 3 examples per class, examples in prompt']  # replace with your actual experiment names\n",
    "\n",
    "# Filter the DataFrame for the chosen experiments\n",
    "filtered_df = summary_gpt_subset[summary_gpt_subset['experiment_name'].isin(chosen_experiments)]\n",
    "\n",
    "# Sort the filtered DataFrame by 'total cost of this experiment' to ensure left-to-right order\n",
    "filtered_df = filtered_df.sort_values(by=['experiment_name', 'total cost of this experiment'])\n",
    "\n",
    "\n",
    "# Define specific colors for models and experiments\n",
    "model_colors = {\n",
    "    'gpt-3.5-turbo-0125': 'blue',  \n",
    "    'gpt-4o': 'orange',         \n",
    "    'gpt-4-turbo': 'red',         \n",
    "    'gpt-4': 'green'               \n",
    "}\n",
    "\n",
    "experiment_colors = {\n",
    "    'Basic zero-shot prompting': 'black',          \n",
    "    'zero-shot cot with legal template': 'crimson',            \n",
    "    'lets think step by step': 'darkorange',\n",
    "    'annollm with 2 examples per class, examples inside prompt':'darkolivegreen',\n",
    "    'few shot with embeddings, per class, three examples per class, in system message': 'indigo',\n",
    "    'few-shot with 3 examples per class, examples in prompt': 'blue' \n",
    "}\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plotting each experiment type with custom line colors\n",
    "for experiment in chosen_experiments:\n",
    "    subset = filtered_df[filtered_df['experiment_name'] == experiment]\n",
    "    plt.plot(subset['total cost of this experiment'], subset['F1_new'], linestyle='-', linewidth=2, color=experiment_colors[experiment], alpha=0.5)\n",
    "    \n",
    "    # Plot each point with custom colors for each model\n",
    "    for model in models:\n",
    "        model_subset = subset[subset['model'] == model]\n",
    "        plt.scatter(model_subset['total cost of this experiment'], model_subset['F1_new'], color=model_colors[model], s=50, alpha = 0.8)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Cost of the Experiment (US Dollars)')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Score vs. Cost of the Experiment')\n",
    "\n",
    "# Show plot without legends\n",
    "plt.grid(False)\n",
    "plt.savefig('cost analysis for gpt models.pdf', format = 'pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6675e97-b4f6-4d32-bac0-54956b807e30",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create an empty plot just for the legend\n",
    "plt.figure(figsize=(4, 2))\n",
    "\n",
    "\n",
    "experiment_name_dict = {'Basic zero-shot prompting': 'Basic zero-shot prompting',          \n",
    "    'zero-shot cot with legal template': 'Legal reasoning template B',            \n",
    "    'lets think step by step': \"Let's think step by step (Zero-shot-CoT)\",\n",
    "    'annollm with 2 examples per class, examples inside prompt':'AnnoLLM',\n",
    "    'few shot with embeddings, per class, three examples per class, in system message': 'Few-shot prompting with embeddings',\n",
    "    'few-shot with 3 examples per class, examples in prompt': 'Simple few-shot prompting' \n",
    "}\n",
    "\n",
    "# Plot dummy lines for each experiment type with custom colors to create the legend\n",
    "for experiment in chosen_experiments:\n",
    "    plt.plot([], [], label=experiment_name_dict[experiment], linestyle='-', linewidth=2, color=experiment_colors[experiment], alpha = 0.5)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(title='Experiment Type', loc='center')\n",
    "\n",
    "# Remove axes\n",
    "plt.axis('off')\n",
    "\n",
    "# Show the experiment legend plot\n",
    "plt.savefig('cost analysis for gpt models legend1.pdf', format = 'pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60283078-7a86-4faf-a48d-d415587503b6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model_names = {\n",
    "    'gpt-3.5-turbo-0125': 'GPT-3.5 Turbo',  \n",
    "    'gpt-4o': 'GPT-4o',         \n",
    "    'gpt-4-turbo': 'GPT-4 Turbo',         \n",
    "    'gpt-4': 'GPT-4'               \n",
    "}\n",
    "\n",
    "# Create an empty plot just for the legend\n",
    "plt.figure(figsize=(3, 2))\n",
    "\n",
    "# Plot dummy markers for each model with custom colors to create the legend\n",
    "for model in models:\n",
    "    plt.plot([], [], color=model_colors[model], marker='o', linestyle='', markersize=8, label=model_names[model], alpha= 0.8)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(title='Models', loc='center')\n",
    "\n",
    "# Remove axes\n",
    "plt.axis('off')\n",
    "plt.savefig('cost analysis for gpt models legend2.pdf', format = 'pdf')\n",
    "# Show the model legend plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d5e367-9513-481f-8938-4876f19d77bb",
   "metadata": {},
   "source": [
    "## cost graphs for appending the contract for GPT-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2dc2a6-52a5-4b7b-a5a7-f4efc75a8fd3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "F1_scores = {'basic zero-shot': 0.48, \"let's think step by step\": 0.41, 'legal reasoning template': 0.55, 'simple few-shot': 0.64, 'few-shot with embeddings': 0.80, 'annoLLM': 0.70, 'providing the source contract': 0.71}\n",
    "costs = {'basic zero-shot': 0.136860, \"let's think step by step\": 0.727845, 'legal reasoning template': 0.705780, 'simple few-shot': 0.850600, 'few-shot with embeddings': 0.513380, 'annoLLM': 1.366335, 'providing the source contract': 9.818725}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded3ef59-56e5-4e4a-8199-a983af160a4e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "9.818725/0.513380"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ace170b-ac65-4d58-8551-f710edcd7d4e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plotting each point\n",
    "for key in F1_scores:\n",
    "    plt.scatter(costs[key], F1_scores[key], label=key)\n",
    "\n",
    "# Adding lines between the points\n",
    "sorted_keys = sorted(costs, key=costs.get)\n",
    "sorted_costs = [costs[key] for key in sorted_keys]\n",
    "sorted_F1_scores = [F1_scores[key] for key in sorted_keys]\n",
    "\n",
    "plt.plot(sorted_costs, sorted_F1_scores, linestyle='-', color='gray', alpha=0.5)\n",
    "\n",
    "# Adding labels and legend\n",
    "plt.xlabel('Cost of the Experiment (US Dollars)')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Scores vs Costs for GPT-4o')\n",
    "plt.legend(title='Prompting Methods')\n",
    "\n",
    "plt.grid(False)\n",
    "plt.savefig('appending the contract cost for gpt4o.pdf', format = 'pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887bef47-1626-4c92-957f-4365189144d5",
   "metadata": {},
   "source": [
    "## Model Size-Performance graphs for the Llama-3 Family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc602f9-35c2-4b6f-b81e-d5f99638f8e4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_models = ['llama3:8b','meta/meta-llama-3-70b-instruct']\n",
    "columns_to_consider = ['experiment_name', 'model', 'F1_new','experiment ran for (seconds)']\n",
    "unique_values = {col: [] for col in columns_to_consider}\n",
    "\n",
    "# Iterate through each DataFrame and collect unique values\n",
    "for df in updated_results:\n",
    "    if df['model'][0] in llama_models:\n",
    "        df_llama = df[df['model'].isin(llama_models)]\n",
    "        for col in columns_to_consider:\n",
    "            unique_val = df_llama[col].unique()\n",
    "            if len(unique_val) == 1:  # Ensure it's a unique value column\n",
    "                unique_values[col].append(unique_val[0])\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "summary_llama_subset = pd.DataFrame(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07d5efd-526a-41c1-b094-7b84acf320ba",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary_llama_subset['group'] = summary_llama_subset.index // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6523a5b6-7df0-429c-ae1e-242e342f21f4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary_llama_subset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3174134-bd5b-460b-a2c2-151267eeb63d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_grouped = summary_llama_subset.groupby('group')['F1_new'].mean().reset_index()\n",
    "\n",
    "df_grouped.columns = ['group', 'avg F1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fd683d-76e2-44ef-95c2-5ee53aa953be",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = ['meta/meta-llama-3-70b-instruct', 'llama3:8b']\n",
    "chosen_experiments = ['Basic zero-shot prompting', 'lets think step by step', 'zero-shot cot with alex template', 'few-shot with 3 examples per class',\\\n",
    "                      'few shot with embeddings, per class, three examples per class, in system message', 'annollm with 2 examples per class, examples inside prompt']\n",
    "\n",
    "\n",
    "experiment_name_dict = {'Basic zero-shot prompting': 'Basic zero-shot prompting',          \n",
    "    'zero-shot cot with alex template': 'Legal reasoning template A',            \n",
    "    'lets think step by step': \"Let's think step by step (Zero-shot-CoT)\",\n",
    "    'annollm with 2 examples per class, examples inside prompt':'AnnoLLM',\n",
    "    'few shot with embeddings, per class, three examples per class, in system message': 'Few-shot prompting with embeddings',\n",
    "    'few-shot with 3 examples per class': 'Simple few-shot prompting' \n",
    "}\n",
    "\n",
    "model_name_dict = {'meta/meta-llama-3-70b-instruct': 'Llama-3 70B',\n",
    "                   'llama3:8b': 'Llama-3 8B'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad12395e-20c4-4556-b3de-49c881bea4bc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_df = summary_llama_subset[\n",
    "    (summary_llama_subset['model'].isin(models)) &\n",
    "    (summary_llama_subset['experiment_name'].isin(chosen_experiments))\n",
    "]\n",
    "\n",
    "filtered_df['experiment_name'] = filtered_df['experiment_name'].map(experiment_name_dict)\n",
    "filtered_df['model'] = filtered_df['model'].map(model_name_dict)\n",
    "filtered_df['experiment_name'] = pd.Categorical(filtered_df['experiment_name'], categories=[experiment_name_dict[exp] for exp in chosen_experiments[::-1]], ordered=True)\n",
    "\n",
    "# Pivot the dataframe to have models as columns, experiments as rows, and F1_new as values\n",
    "pivot_df = filtered_df.pivot_table(index='experiment_name', columns='model', values='F1_new')\n",
    "pivot_df = pivot_df[[model_name_dict[model] for model in models]]\n",
    "\n",
    "colors = ['mediumpurple','indigo']\n",
    "\n",
    "# Plot the horizontal bar chart\n",
    "ax = pivot_df.plot(kind='barh', figsize=(12,8), color=colors)\n",
    "\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, label_type='edge', fmt='%.2f', padding=1)\n",
    "\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('F1 Score')\n",
    "plt.ylabel('Experiment')\n",
    "plt.title('F1 Scores by Experiment and Model for Llama-3 Family')\n",
    "\n",
    "# Display the plot\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.savefig('performance model size for llama3.pdf', format = 'pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c39f509-e839-4448-af40-49c0987e2d51",
   "metadata": {
    "tags": []
   },
   "source": [
    "## qualitative analysis I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f062c5-3967-4e5d-b9e5-b46db0d92c83",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(updated_results)):\n",
    "    df = results[i]\n",
    "    print(f\"{i}, {df['experiment_name'][0]}, {df['model'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95212b5a-813c-42d9-b56f-138a869e3e82",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama70_A = updated_results[195]\n",
    "llama70_B = updated_results[204]\n",
    "llama70_step = updated_results[186]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d74ce2-8011-4bb3-88e6-4e65883255ef",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama70_A.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de25aef-0cbf-4bee-b098-02470e8f7f8b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First, merge llama70_A with llama70_B\n",
    "merged_df = pd.merge(llama70_A, llama70_B, on=['model', 'clause', 'ground_truth_label'], suffixes=('_A', '_B'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6162ea9d-0109-4d35-a950-9c9327cba9b8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_df = merged_df[merged_df['better_cleaned_A'] != merged_df['better_cleaned_B']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4093757-cb45-4e49-a562-43c847254838",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama70_A_vs_B = filtered_df[['model', 'clause' ,'experiment_name_A', 'experiment_name_B' ,'system_content_A', 'system_content_B', \n",
    "                              'prompt_A', 'prompt_B', 'ground_truth_label', \n",
    "                              'model_output_A', 'model_output_B',\n",
    "                              'better_cleaned_A', 'better_cleaned_B']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180c2091-d9ba-4cef-a046-214f2b5c2b66",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "count = llama70_A_vs_B['better_cleaned_A'].value_counts().get('none', 0)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7b5530-121a-47dc-86a7-a6b12b00b904",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "count = (llama70_A_vs_B['ground_truth_label'] != llama70_A_vs_B['better_cleaned_B']).sum()\n",
    "print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfdf13c-fc9f-4d23-ae97-c6b9ae1df399",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "different_rows = llama70_A_vs_B[llama70_A_vs_B['ground_truth_label'] != llama70_A_vs_B['better_cleaned_A']]\n",
    "different_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78eeb78-a0ed-4d6e-b8ed-5445cf1ec1fa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "count = (llama70_A_vs_B['ground_truth_label'] != llama70_A_vs_B['cleaned_prediction_A']).sum()\n",
    "print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efaaf4d-6a1c-4122-91d5-649e40f8c305",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama70_A_vs_B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda8e5e8-1639-4e62-93cf-005c15a731b0",
   "metadata": {},
   "source": [
    "## qualitative analysis II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4cd138-09ce-457c-8516-d4f141e0ad3d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama8_B = updated_results[203]\n",
    "llama8_S = updated_results[185]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd191f2-a1bf-4462-b01b-9aa0514c0687",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_df = pd.merge(llama8_B, llama8_S, on=['model', 'clause', 'ground_truth_label'], suffixes=('_B', '_S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2b4022-9752-44f1-ae13-3ff3c4b839fe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_df = merged_df[merged_df['better_cleaned_B'] != merged_df['better_cleaned_S']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ad481a-785e-4c38-80ef-4b1c93322c8e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama8_B_vs_S = filtered_df[['model', 'clause' ,'experiment_name_B', 'experiment_name_S' ,'system_content_B', 'system_content_S', \n",
    "                              'prompt_B', 'prompt_S', 'ground_truth_label', \n",
    "                              'model_output_B', 'model_output_S',\n",
    "                              'better_cleaned_B', 'better_cleaned_S']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922f3c1a-7aca-4bb0-9db3-e965fcc9fd46",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama8_B_vs_S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250a644b-e768-4355-93d6-bb041a7a5be9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "count = (llama8_B['better_cleaned'] == 'red flag').sum()\n",
    "print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ef3ba5-fa43-401d-b2a9-aa8417e1977b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama8_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d53bb65-3b4b-4728-a7af-0830318d9cbc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama8_S"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a54224-0fd7-4756-9162-f48001b7743b",
   "metadata": {},
   "source": [
    "## qualitative analysis III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9c1dd8-a04b-4d90-af53-170d31d752aa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpt4turbo_A = updated_results[191]\n",
    "gpt4turbo_B = updated_results[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf24560-1097-4559-b0d8-211a27506695",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_df = pd.merge(gpt4turbo_A, gpt4turbo_B, on=['model', 'clause', 'ground_truth_label'], suffixes=('_A', '_B'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddb56aa-3716-4353-bc28-ffc85b20bf72",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_df = merged_df[merged_df['better_cleaned_A'] != merged_df['better_cleaned_B']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07e5bbd-c07a-4025-a686-5f078865be9c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpt4turbo_A_vs_B = filtered_df[['model', 'clause' ,'experiment_name_A', 'experiment_name_B' ,'system_content_A', 'system_content_B', \n",
    "                              'prompt_A', 'prompt_B', 'ground_truth_label', \n",
    "                              'model_output_A', 'model_output_B',\n",
    "                              'better_cleaned_A', 'better_cleaned_B']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e9aa1d-a1b2-411a-a154-cddfd7078e63",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpt4turbo_A_vs_B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9698cc57-0c68-44fa-8e4b-d896231896bf",
   "metadata": {},
   "source": [
    "## analysis of continued pre-training improvements wrt different contract types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975a9175-1bfb-4eb7-9f05-062c1988eaab",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "folder_path = 'test with further pre-trained results'\n",
    "\n",
    "results_pretrain = []\n",
    "\n",
    "pickle_files = sorted([f for f in os.listdir(folder_path) if f.endswith('.pkl')])\n",
    "\n",
    "for filename in pickle_files:\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    with open(file_path, 'rb') as file:\n",
    "        result = pickle.load(file)\n",
    "        results_pretrain.append(result)\n",
    "\n",
    "\n",
    "N = 200\n",
    "\n",
    "updated_results_pretrain = []\n",
    "for i in range(len(results_pretrain)):\n",
    "    dataframe = pd.DataFrame(results_pretrain[i])\n",
    "    experiment_name = dataframe['experiment_name'][0]\n",
    "    if 'step by step' in experiment_name or 'appending' in experiment_name:\n",
    "        dataframe['better_cleaned'] = dataframe['model_output'].apply(cleanup_result_first_new)\n",
    "        dataframe['old_cleaned'] = dataframe['model_output'].apply(cleanup_result_first)\n",
    "    elif 'alex' in experiment_name:\n",
    "        dataframe['old_cleaned'] = dataframe['model_output'].apply(cleanup_result_first)\n",
    "        dataframe['better_cleaned'] = dataframe['model_output'].apply(cleanup_result_after_result)\n",
    "        \n",
    "    else:\n",
    "        dataframe['better_cleaned'] = dataframe['model_output'].apply(cleanup_result_last_new)\n",
    "        dataframe['old_cleaned'] = dataframe['model_output'].apply(cleanup_result_last)\n",
    "    \n",
    "    new_cleaned_results = dataframe['better_cleaned'].tolist()\n",
    "    #print(new_cleaned_results)\n",
    "    old_cleaned_results = dataframe['old_cleaned'].tolist()\n",
    "    \n",
    "    new_metrics = metrics_mine_dict(test_risks[:N], new_cleaned_results)\n",
    "    old_metrics = metrics_mine_dict(test_risks[:N], old_cleaned_results)\n",
    "    #newer_metrics = metrics_mine_dict_again(test_risks[:N], new_cleaned_results)\n",
    "        \n",
    "    \n",
    "    dataframe['accuracy_new'] = new_metrics['Accuracy']\n",
    "    dataframe['precision_new'] = new_metrics['Precision']\n",
    "    dataframe['recall_new'] = new_metrics['Recall']\n",
    "    dataframe['F1_new'] = new_metrics['F1']\n",
    "    \n",
    "    dataframe['accuracy_old'] = old_metrics['Accuracy']\n",
    "    dataframe['precision_old'] = old_metrics['Precision']\n",
    "    dataframe['recall_old'] = old_metrics['Recall']\n",
    "    dataframe['F1_old'] = old_metrics['F1']\n",
    "    \n",
    "       \n",
    "    updated_results_pretrain.append(dataframe)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cc88b7-3a07-46bd-922b-4a7b1160ba5e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "updated_results_pretrain2 = []\n",
    "for i in range(len(updated_results_pretrain)):\n",
    "    if updated_results_pretrain[i]['model'][0] in ['llama3:8b', 'unsloth_model_1_epoch:latest']:\n",
    "        updated_results_pretrain2.append(updated_results_pretrain[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1671a32-ed93-4f9c-88f9-60574bf6d0e5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(updated_results_pretrain2)):\n",
    "    print(updated_results_pretrain2[i]['experiment_name'][0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8909cb62-7270-4d8f-88a5-b9f8b93b9685",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "grouped = {}\n",
    "for df in updated_results_pretrain2:\n",
    "    key = (df['experiment_name'].iloc[0], df['model'].iloc[0])\n",
    "    grouped[key] = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13b9749-1d15-4bcc-b6d0-4d2cb1203b1f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "contract_type_colors = {\n",
    "    'implementation agreement': 'powderblue',\n",
    "    'terms and conditions': 'lightcoral',\n",
    "    'vendor agreement': 'lightblue',\n",
    "    'non-disclosure agreement': 'lightgoldenrodyellow',\n",
    "    'SaaS (software as a service)':'thistle'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375d12f0-130f-40c9-a43b-cb5ba0ed5f87",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiment_name_dict = {'few-shot with 2 examples per class' : 'Simple Few-shot Prompting',\n",
    "                       'zero-shot cot with legal template': 'Zero-shot with Legal Reasoning Template B',\n",
    "                       'few shot with embeddings, per class, two examples per class, in system message': 'Few-shot Prompting with Embeddings',\n",
    "                        'annollm with 3 examples per class, examples inside system message': 'AnnoLLM'\n",
    "                       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6212019-db21-43ae-8b10-1789387a7ba1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def final_exp_name(experiment_name):\n",
    "    if experiment_name in list(experiment_name_dict.keys()):\n",
    "        final_exp_name = experiment_name_dict[experiment_name]\n",
    "    else:\n",
    "        final_exp_name = experiment_name\n",
    "    return final_exp_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630963d9-cffb-4c2d-a59f-5bb10c50f417",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for experiment_name in set([key[0] for key in grouped.keys()]):\n",
    "    df_A = grouped[(experiment_name, 'llama3:8b')] # base model\n",
    "    df_B = grouped[(experiment_name, 'unsloth_model_1_epoch:latest')] # continued pre-trained model\n",
    "\n",
    "    if df_B['F1_new'].iloc[0] > df_A['F1_new'].iloc[0]:\n",
    "        print(df_B['experiment_name'][0])\n",
    "        # Filter rows where Model B is correct and Model A is wrong\n",
    "        correct_B_wrong_A = df_B[(df_B['better_cleaned'] == df_B['ground_truth_label']) & \n",
    "                                 (df_A['better_cleaned'] != df_A['ground_truth_label'])]\n",
    "        \n",
    "        # Count contract types for the filtered rows\n",
    "        contract_type_counts = correct_B_wrong_A['contract_type'].value_counts()\n",
    "        \n",
    "        colors = [contract_type_colors[contract_type] for contract_type in contract_type_counts.index]\n",
    "\n",
    "\n",
    "        # Create a pie chart\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.pie(contract_type_counts, labels=contract_type_counts.index, autopct='%1.1f%%', startangle=140, colors = colors, textprops={'fontsize': 7})\n",
    "        plt.title(f\"{final_exp_name(experiment_name)}\")\n",
    "        if experiment_name in list(experiment_name_dict.keys()):\n",
    "            plt.savefig(f\"pie chart for dists {final_exp_name(experiment_name)}.pdf\", format = 'pdf')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1378a0b3-f7fd-4277-8c74-c9c6b2b0d999",
   "metadata": {},
   "source": [
    "## analysis of similarity scores for few-shot with embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b744378-6fc1-4eb0-86e6-9a77b4f37d89",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ic_examples = pd.read_csv('ic_examples_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03662923-c9c8-4a08-ab5a-6627725f0699",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def choose_in_context_examples_2_score(clause_to_be_tested, df, n_samples):\n",
    "    def parse_embedding(embedding_str):\n",
    "        return np.array(ast.literal_eval(embedding_str), dtype=np.float32)\n",
    "    \n",
    "    # split the df according to risk\n",
    "    red_flag_df = df[df['ground_truth_label'] == 'red flag']\n",
    "    potential_issue_df = df[df['ground_truth_label'] == 'potential issue']\n",
    "\n",
    "    # list the embeddings \n",
    "    test_embeddings_red_flag = np.vstack(red_flag_df['embedding'].apply(parse_embedding).values)\n",
    "    test_embeddings_potential_issue = np.vstack(potential_issue_df['embedding'].apply(parse_embedding).values)\n",
    "    \n",
    "    # get the embedding of the clause to be tested\n",
    "    clause_embed_ = df_test.loc[df_test['clause'] == clause_to_be_tested, 'embedding']\n",
    "    clause_embed = parse_embedding(clause_embed_.values[0])  # Convert to numpy array\n",
    "    \n",
    "    # find similarity scores\n",
    "    similarities_red_flag = np.dot(test_embeddings_red_flag, clause_embed)\n",
    "    similarities_potential_issue = np.dot(test_embeddings_potential_issue, clause_embed)\n",
    "\n",
    "    top_similarities_red_flag = np.partition(similarities_red_flag, -n_samples)[-n_samples:]\n",
    "    top_similarities_potential_issue = np.partition(similarities_potential_issue, -n_samples)[-n_samples:]\n",
    "\n",
    "    top_similarities_red_flag = np.sort(top_similarities_red_flag)[::-1]\n",
    "    top_similarities_potential_issue = np.sort(top_similarities_potential_issue)[::-1]\n",
    "    \n",
    "    return top_similarities_potential_issue, top_similarities_red_flag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1a0a40-96b6-4298-afaf-8d21f6b968fe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_similarity_dataframe(test_clauses, df, n_samples, n_random):\n",
    "    # Randomly select n_random clauses\n",
    "    random_indices = np.random.choice(len(test_clauses), n_random, replace=False)\n",
    "    selected_clauses = [test_clauses[i] for i in random_indices]\n",
    "    \n",
    "    # Initialize a list to hold the rows of the DataFrame\n",
    "    rows = []\n",
    "    \n",
    "    for i, clause in enumerate(selected_clauses):\n",
    "        top_potential_issue, top_red_flag = choose_in_context_examples_2_score(clause, df, n_samples)\n",
    "        row = [random_indices[i]] + top_red_flag.tolist() + top_potential_issue.tolist()\n",
    "        rows.append(row)\n",
    "    \n",
    "    # Create column names\n",
    "    col_names = ['test_datum_index'] + \\\n",
    "                [f'ic_example_red_flag_{i+1}' for i in range(n_samples)] + \\\n",
    "                [f'ic_example_potential_issue_{i+1}' for i in range(n_samples)]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    similarity_df = pd.DataFrame(rows, columns=col_names)\n",
    "    \n",
    "    similarity_df.to_csv('similarity_dataframe.csv', index=False)\n",
    "\n",
    "    return similarity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04167bc3-1884-45ef-80b2-99e89aef0c80",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_similarity_dataframe(test_clauses, df_ic_examples, 3, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92109fb4-005b-4ceb-8138-89e0b22010a8",
   "metadata": {},
   "source": [
    "## qualitative analysis IV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f48169-8bcf-45c1-b0cf-bb2ac913bf13",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "template_a_8b = updated_results[194]\n",
    "template_a_70b = updated_results[195]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71267348-76ab-48c1-b024-787a6752fd2a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "template_a_8b.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a12831-8f2c-484f-9678-7a2fa2908865",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_df_template_a = pd.merge(template_a_8b, template_a_70b, on=['experiment_name', 'clause', 'ground_truth_label'], suffixes=('_8', '_70'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ebb1b6-eb2e-469a-831f-1b4c3b0648f4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_df_template_a = merged_df_template_a[merged_df_template_a['better_cleaned_8'] != merged_df_template_a['better_cleaned_70']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014cfde2-1f95-4047-87f4-d624c8ee653b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "template_a_8_vs_70 = filtered_df_template_a[['experiment_name', 'model_8', 'model_70', 'system_content_8', 'system_content_70', \n",
    "                              'prompt_8', 'prompt_70', 'ground_truth_label', \n",
    "                              'better_cleaned_8', 'better_cleaned_70']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91be01b0-26a8-4cd4-b3b3-69294c44a774",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(filtered_df_template_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd1fba2-1910-4f5b-8bc1-4b081ba2507a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "count = template_a_70b['better_cleaned'].value_counts().get('none', 0)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6737675e-4d09-4f27-9019-a93d72a7c4c2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "count = (template_a_8_vs_70['ground_truth_label'] != template_a_8_vs_70['better_cleaned_70']).sum()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b48207-c9f2-4f14-b741-077aca3b6036",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d0955d-06aa-4b71-aa3e-e93b0e4023ab",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lets_think_8b = updated_results[185]\n",
    "lets_think_70b = updated_results[186]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25c4f6a-9907-4236-996e-10139938b9db",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_df_lets_think = pd.merge(lets_think_8b, lets_think_70b, on=['experiment_name', 'clause', 'ground_truth_label'], suffixes=('_8', '_70'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515cf3dd-ae24-4716-a78c-038c2614b3f9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_df_lets_think = merged_df_lets_think[merged_df_lets_think['better_cleaned_8'] != merged_df_lets_think['better_cleaned_70']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69509fa7-b5ab-4659-835a-7944fd7c3980",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(filtered_df_lets_think)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5277d89-8d25-4a7a-ac89-2177cbf662d2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lets_think_8_vs_70 = filtered_df_lets_think[['experiment_name', 'clause', 'system_content_8', 'system_content_70', \n",
    "                              'prompt_8', 'prompt_70', 'ground_truth_label', 'model_output_8', 'model_output_70']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09b3d4a-c083-4ddd-bbc3-f09333a2557e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lets_think_8_vs_70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560e7c0d-3fe5-4f85-b447-3de019572fb2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "count = lets_think_70b['better_cleaned'].value_counts().get('none', 0)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a71ea1-6456-453e-9e19-e37fa6a65abf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "count = (lets_think_8_vs_70['ground_truth_label'] != lets_think_8_vs_70['better_cleaned_70']).sum()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6854c2e0-ae98-4553-9824-95608396d9c2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "count = lets_think_70b['better_cleaned'].value_counts().get('potential issue', 0)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52cc4bb-c5df-4716-803f-455a9906e621",
   "metadata": {},
   "source": [
    "# analysis for continued pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dc1327-a763-4810-a62b-02fdaa2e850c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "folder_path = 'test with further pre-trained results'\n",
    "\n",
    "results = []\n",
    "\n",
    "pickle_files = sorted([f for f in os.listdir(folder_path) if f.endswith('.pkl')])\n",
    "\n",
    "for filename in pickle_files:\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    with open(file_path, 'rb') as file:\n",
    "        result = pickle.load(file)\n",
    "        results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e2d296-648a-41ab-99f2-60bc1554f53f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N = 200\n",
    "\n",
    "updated_results = []\n",
    "for i in range(len(results)):\n",
    "    dataframe = pd.DataFrame(results[i])\n",
    "    experiment_name = dataframe['experiment_name'][0]\n",
    "    if 'step by step' in experiment_name or 'appending' in experiment_name:\n",
    "        dataframe['better_cleaned'] = dataframe['model_output'].apply(cleanup_result_first_new)\n",
    "    elif 'alex' in experiment_name:\n",
    "        dataframe['better_cleaned'] = dataframe['model_output'].apply(cleanup_result_after_result)\n",
    "        \n",
    "    else:\n",
    "        dataframe['better_cleaned'] = dataframe['model_output'].apply(cleanup_result_last_new)\n",
    "    \n",
    "    new_cleaned_results = dataframe['better_cleaned'].tolist()\n",
    "    \n",
    "    new_metrics = metrics_mine_dict(test_risks[:N], new_cleaned_results)\n",
    "    \n",
    "    dataframe['accuracy_new'] = new_metrics['Accuracy']\n",
    "    dataframe['precision_new'] = new_metrics['Precision']\n",
    "    dataframe['recall_new'] = new_metrics['Recall']\n",
    "    dataframe['F1_new'] = new_metrics['F1']\n",
    "\n",
    "    updated_results.append(dataframe)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2a6721-99ef-4ed2-972d-f06691046fc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#columns_to_consider = ['experiment_name', 'model', 'F1_new','total cost of this experiment','experiment ran for (seconds)']\n",
    "columns_to_consider = ['experiment_name', 'model', 'F1_new','experiment ran for (seconds)']\n",
    "unique_values = {col: [] for col in columns_to_consider}\n",
    "\n",
    "# Iterate through each DataFrame and collect unique values\n",
    "for df in updated_results:\n",
    "    for col in columns_to_consider:\n",
    "        unique_val = df[col].unique()\n",
    "        if len(unique_val) == 1:  # Ensure it's a unique value column\n",
    "            unique_values[col].append(unique_val[0])\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "summary_continued = pd.DataFrame(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccd1094-00b8-434f-91fa-b99796df2bf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary_continued"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefafe40-3d09-4c4b-b468-6f6decf4b15e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "average_f1_per_model = summary_continued.groupby('model')['F1_new'].mean().reset_index()\n",
    "average_f1_per_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad26358-5edc-44ff-81d9-7af21562799b",
   "metadata": {},
   "source": [
    "## 1 epoch vs 3 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788b17e7-70ec-4c91-814e-5dc1961c8d9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = ['unsloth_model_1_epoch:latest', 'unsloth_model_3_epochs:latest']\n",
    "summary_epoch = summary_continued[summary_continued['model'].isin(models)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378d1aae-44f2-409e-8f0a-61ddfd0156b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pivot_table = summary_epoch.pivot(index='model', columns='experiment_name', values='F1_new')\n",
    "pivot_table.reset_index(inplace=True)\n",
    "\n",
    "pivot_table"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m123"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
